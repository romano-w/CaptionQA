{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05ba053",
   "metadata": {},
   "source": [
    "# **360x Dataset EDA**\n",
    "\n",
    "Quick exploratory checks on the 360x panoramic dataset: verify structure, inspect sample entries, and capture basic summary stats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd72fae",
   "metadata": {},
   "source": [
    "### Dataset context\n",
    "- The [360+x project page](https://x360dataset.github.io/) describes the dataset as a **panoptic multi-modal scene understanding** corpus with 2,152 videos (8.579M frames) captured using 360° and Spectacles cameras across 17 cities in 5 countries, covering 28 scene categories spanning indoor and outdoor environments.\n",
    "- Each video carries **temporal activity localization for 38 action classes**, synchronized **binaural audio** (with published delay statistics), and aligned third-person, panoramic, and binocular viewpoints to encourage cross-modal analysis.\n",
    "- The downloadable packages on Hugging Face include both **high-resolution assets** (panoramic 5760×2880, binocular 2432×1216, front-view 1920×1080) and **lower-resolution variants** (panoramic/binocular 640×320, front-view 569×320), with shared JSON indices, class maps, and per-clip activity segmentation metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59231e7",
   "metadata": {},
   "source": [
    "## 1. Resolve dataset root\n",
    "\n",
    "Uses the same resolution logic as the quickstart notebook: prefer `CAPTIONQA_DATASETS`, otherwise search upward for the repository root and fall back to `./datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7b773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: D:\\CaptionQA\\data\n",
      "HR root: D:\\CaptionQA\\data\\360x\\360x_dataset_HR\n",
      "LR root: D:\\CaptionQA\\data\\360x\\360x_dataset_LR\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "import json\n",
    "import os\n",
    "\n",
    "def resolve_dataset_root() -> Path:\n",
    "    env_root = os.environ.get('CAPTIONQA_DATASETS')\n",
    "    if env_root:\n",
    "        return Path(env_root).expanduser().resolve()\n",
    "    cwd = Path.cwd()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        if (candidate / 'pyproject.toml').exists() or (candidate / '.git').exists():\n",
    "            return (candidate / 'datasets').resolve()\n",
    "    return (cwd / 'datasets').resolve()\n",
    "\n",
    "DATASET_ROOT = resolve_dataset_root()\n",
    "print(f'Dataset root: {DATASET_ROOT}')\n",
    "HR_ROOT = DATASET_ROOT / '360x' / '360x_dataset_HR'\n",
    "LR_ROOT = DATASET_ROOT / '360x' / '360x_dataset_LR'\n",
    "print(f'HR root: {HR_ROOT}')\n",
    "print(f'LR root: {LR_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ad13d",
   "metadata": {},
   "source": [
    "## 2. Validate presence & summarize\n",
    "\n",
    "Run the next cell to verify the expected folder layout and capture a few file listings. The notebook handles missing data gracefully and prints next steps if the dataset is absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e306c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High-resolution split -> D:\\CaptionQA\\data\\360x\\360x_dataset_HR\n",
      "Total entries: 5\n",
      " - .cache\n",
      " - .gitattributes\n",
      " - binocular\n",
      " - README.md\n",
      " - TAL_annotations\n",
      "\n",
      "Low-resolution split -> D:\\CaptionQA\\data\\360x\\360x_dataset_LR\n",
      "[missing] Not found. Download with: python -m captionqa.data.download 360x --output D:\\CaptionQA\\data\n"
     ]
    }
   ],
   "source": [
    "def describe_directory(root: Path, label: str, limit: int = 10):\n",
    "    print(f'\\n{label} -> {root}')\n",
    "    if not root.exists():\n",
    "        print('[missing] Not found. Download with: python -m captionqa.data.download 360x --output', DATASET_ROOT)\n",
    "        return\n",
    "    entries = sorted(root.iterdir())\n",
    "    print(f'Total entries: {len(entries)}')\n",
    "    for path in islice(entries, limit):\n",
    "        print(' -', path.name)\n",
    "    if len(entries) > limit:\n",
    "        print(' ...')\n",
    "\n",
    "describe_directory(HR_ROOT, 'High-resolution split')\n",
    "describe_directory(LR_ROOT, 'Low-resolution split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792ab4c",
   "metadata": {},
   "source": [
    "## 3. Sample metadata (optional)\n",
    "\n",
    "If JSON metadata files are available, the next cell attempts to load one sample to inspect structure. Adjust the glob pattern if the dataset uses a different naming scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762e86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing D:\\CaptionQA\\data\\360x\\360x_dataset_HR\\TAL_annotations\\019cc67f-512f-4b8a-96ef-81f806c86ce1.json\n",
      "{\n",
      "  \"file\": {\n",
      "    \"1\": {\n",
      "      \"fid\": \"1\",\n",
      "      \"fname\": \"360_panoramic.mp4\",\n",
      "      \"type\": 4,\n",
      "      \"loc\": 1,\n",
      "      \"src\": \"\"\n",
      "    }\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"1_1_1\": {\n",
      "      \"duration\": [\n",
      "        5.516,\n",
      "        7.3905\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"sitting\"\n",
      "      }\n",
      "    },\n",
      "    \"1_2_2\": {\n",
      "      \"duration\": [\n",
      "        12.312,\n",
      "        17.47384\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"drinking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_2_3\": {\n",
      "      \"duration\": [\n",
      "        51.417,\n",
      "        56.46075\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"drinking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_2_4\": {\n",
      "      \"duration\": [\n",
      "        235.334,\n",
      "        237.98142\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"drinking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_5\": {\n",
      "      \"duration\": [\n",
      "        9.558,\n",
      "        116.33559\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_6\": {\n",
      "      \"duration\": [\n",
      "        135.211,\n",
      "        167.50226\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_7\": {\n",
      "      \"duration\": [\n",
      "        174.288,\n",
      "        232.31476\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_8\": {\n",
      "      \"duration\": [\n",
      "        240.065,\n",
      "        271.08559\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_9\": {\n",
      "      \"duration\": [\n",
      "        274.359,\n",
      "        324.34302\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_4_10\": {\n",
      "      \"duration\": [\n",
      "        115.656,\n",
      "        313.96059\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"operating phone\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "[skip] Root missing; nothing to load.\n"
     ]
    }
   ],
   "source": [
    "def load_sample_metadata(root: Path, suffix: str = '.json'):\n",
    "    if not root.exists():\n",
    "        print('[skip] Root missing; nothing to load.')\n",
    "        return\n",
    "    for path in sorted(root.rglob(f'*{suffix}')):\n",
    "        print(f'Previewing {path}')\n",
    "        try:\n",
    "            with path.open('r', encoding='utf-8') as handle:\n",
    "                snippet = json.load(handle)\n",
    "        except Exception as exc:\n",
    "            print('Failed to parse JSON:', exc)\n",
    "            return\n",
    "        print(json.dumps(snippet, indent=2)[:2000])\n",
    "        return\n",
    "    print('[skip] No files matching suffix found.')\n",
    "\n",
    "load_sample_metadata(HR_ROOT)\n",
    "load_sample_metadata(LR_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c42cd",
   "metadata": {},
   "source": [
    "## 4. Detailed roadmap \n",
    "\n",
    "### A. Data access & integrity audit\n",
    "1. **Confirm environment authentication**\n",
    "   - Verify `huggingface-cli whoami` succeeds so gated downloads remain accessible.\n",
    "   - Mirror the final dataset root (HR vs LR) and capture the exact path in the notebook for reproducibility.\n",
    "2. **Inventory archives & modality folders**\n",
    "   - Programmatically list `panoramic/`, `binocular/`, `third_person/`, and `activity_segmentation/` directories; assert counts match published totals (2,152 full videos / 1,380 clips) once decompressed.\n",
    "   - Parse `index.json` to extract per-item metadata (scene id, city, capture device, clip ids) and ensure all referenced files exist.\n",
    "3. **Checksum / size validation (spot checks)**\n",
    "   - Compute file sizes & optional hashes for a random stratified sample to confirm downloads are complete across modalities and resolutions.\n",
    "\n",
    "### B. Metadata profiling\n",
    "1. **Scene & geography coverage**\n",
    "   - Load `index.json` into a DataFrame; summarize counts by `scene_category`, `city`, `country`, indoor/outdoor flag to verify the 28-scene, 17-city distribution.\n",
    "   - Visualize distributions (bar charts, choropleth-ready tables) and highlight underrepresented categories.\n",
    "2. **Action label analysis**\n",
    "   - Iterate `activity_segmentation/*.json`; explode temporal segments to calculate frequency, duration, and co-occurrence of the 38 action classes.\n",
    "   - Plot duration histograms and per-video action counts to reproduce/double-check dataset charts (e.g., number of actions per clip, time-of-day coverage).\n",
    "3. **Clip segmentation review**\n",
    "   - Confirm 1,380 ~10s clips by aggregating segment metadata, checking total duration (~244k seconds / 67.78 hours) against expectations.\n",
    "\n",
    "### C. Video modality diagnostics\n",
    "1. **Resolution & bitrate verification**\n",
    "   - Use `ffprobe` (via `ffmpeg-python` or subprocess) on samples from each modality/resolution to confirm frame size, FPS, codec, audio channels.\n",
    "   - Tabulate metrics to ensure panoramic videos retain 360° projection metadata (e.g., equirectangular tags).\n",
    "2. **Temporal alignment checks**\n",
    "   - For matching clip IDs across panoramic/front-view/binocular videos, compute start/end timestamps and verify synchronization with activity segments.\n",
    "   - Overlay representative frames to inspect spatial correspondence between modalities.\n",
    "3. **Quality spotlights**\n",
    "   - Render thumbnails or short GIFs for a stratified sample (scene type × device) to visually inspect exposure, motion blur, and unique scenarios.\n",
    "\n",
    "### D. Audio & binaural analysis\n",
    "1. **Channel inspection**\n",
    "   - Confirm audio streams are stereo/binaural; measure inter-channel delay statistics to compare with published histograms.\n",
    "2. **Spectrogram profiling**\n",
    "   - Generate Mel spectrograms for random clips to assess frequency coverage; store representative figures in the notebook.\n",
    "3. **Cross-modal cues**\n",
    "   - Correlate audio energy bursts with action segment timestamps to evaluate labeling quality.\n",
    "\n",
    "### E. Feature & annotation validation\n",
    "1. **Pre-computed feature parity**\n",
    "   - If I3D/VGGish/ResNet-18 features are present, verify dimensionality and sample statistics; confirm number of feature files equals number of clips.\n",
    "2. **Class mapping sanity checks**\n",
    "   - Inspect `classes.json` to validate naming consistency between metadata and activity labels; flag missing or duplicate entries.\n",
    "\n",
    "### F. Documentation & reproducibility\n",
    "1. **Record assumptions & gaps**\n",
    "   - Maintain a running log within the notebook capturing any anomalies (missing files, corrupted clips) and remediation steps.\n",
    "2. **Outline next analytical directions**\n",
    "   - Based on findings, prioritize deeper tasks (e.g., pose estimation feasibility, QA pair synthesis), linking them to concrete dataset evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0cf5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willj\\Documents\\Coding Projects\\CaptionQA\\captionqa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face auth: OK - Quiixotic\n",
      "CAPTIONQA_DATASETS = <unset>\n",
      "Using dataset root = D:\\CaptionQA\\data\n",
      "HR root = D:\\CaptionQA\\data\\360x\\360x_dataset_HR\n",
      "LR root = D:\\CaptionQA\\data\\360x\\360x_dataset_LR\n"
     ]
    }
   ],
   "source": [
    "# A.1 Confirm environment authentication and capture dataset root\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "try:\n",
    "    info = HfApi().whoami()\n",
    "    user = info.get('name') or info.get('email') or '<unknown>'\n",
    "    print('Hugging Face auth: OK -', user)\n",
    "except Exception as exc:\n",
    "    print('Hugging Face auth: NOT AUTHENTICATED')\n",
    "    print('Hint: run \"huggingface-cli login\" or set HF_TOKEN before downloads.')\n",
    "\n",
    "print('CAPTIONQA_DATASETS =', os.environ.get('CAPTIONQA_DATASETS', '<unset>'))\n",
    "print('Using dataset root =', DATASET_ROOT)\n",
    "print('HR root =', HR_ROOT)\n",
    "print('LR root =', LR_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592f16a",
   "metadata": {},
   "source": [
    "### A.2 Inventory modality folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a30452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR inventory:\n",
      " - binocular          video          55 @ D:\\CaptionQA\\data\\360x\\360x_dataset_HR\\binocular\n",
      " - TAL_annotations    annotations    232 @ D:\\CaptionQA\\data\\360x\\360x_dataset_HR\\TAL_annotations\n",
      "LR inventory:\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def inventory_videos(root: Path):\n",
    "    report = {}\n",
    "    if not root.exists():\n",
    "        return report\n",
    "    candidates = ['binocular', 'panoramic', 'front_view', 'third_person', 'activity_segmentation', 'TAL_annotations']\n",
    "    for name in candidates:\n",
    "        path = root / name\n",
    "        if path.exists():\n",
    "            if name.lower() in ('tal_annotations', 'activity_segmentation'):\n",
    "                count = len(list(path.glob('*.json')))\n",
    "                report[name] = {'type': 'annotations', 'count': count, 'path': str(path)}\n",
    "            else:\n",
    "                vids = list(path.rglob('*.mp4'))\n",
    "                report[name] = {'type': 'video', 'count': len(vids), 'path': str(path)}\n",
    "    if not report:\n",
    "        vids = list(root.rglob('*.mp4'))\n",
    "        report['all_videos'] = {'type': 'video', 'count': len(vids), 'path': str(root)}\n",
    "    return report\n",
    "\n",
    "print('HR inventory:')\n",
    "hr_inv = inventory_videos(HR_ROOT)\n",
    "for k, v in hr_inv.items():\n",
    "    print(f\" - {k:<18} {v['type']:<10} {v['count']:>6} @ {v['path']}\")\n",
    "\n",
    "print('LR inventory:')\n",
    "lr_inv = inventory_videos(LR_ROOT)\n",
    "for k, v in lr_inv.items():\n",
    "    print(f\" - {k:<18} {v['type']:<10} {v['count']:>6} @ {v['path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a04d47",
   "metadata": {},
   "source": [
    "### A.3 Parse index/classes (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c60d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] No index.json or classes.json found under HR root.\n"
     ]
    }
   ],
   "source": [
    "candidates = list(HR_ROOT.rglob('index.json')) + list(HR_ROOT.rglob('classes.json'))\n",
    "if not candidates:\n",
    "    print('[info] No index.json or classes.json found under HR root.')\n",
    "else:\n",
    "    for p in candidates[:5]:\n",
    "        try:\n",
    "            with p.open('r', encoding='utf-8') as h:\n",
    "                obj = json.load(h)\n",
    "        except Exception as exc:\n",
    "            print('Failed to parse', p, '->', exc)\n",
    "            continue\n",
    "        keys = list(obj) if isinstance(obj, dict) else (list(obj[0].keys()) if isinstance(obj, list) and obj else [])\n",
    "        print(f'Parsed {p} | keys: {keys[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e123e",
   "metadata": {},
   "source": [
    "### A.4 Spot-check file sizes and hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd20268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binocular samples (3):\n",
      " - clip4.mp4                                size=109,447,419B sha256[:8]=bd176b2e scanned=5,242,880B\n",
      " - clip3.mp4                                size=8,236,777B sha256[:8]=7a2c5197 scanned=5,242,880B\n",
      " - clip1.mp4                                size=217,734,288B sha256[:8]=f6a99fe4 scanned=5,242,880B\n"
     ]
    }
   ],
   "source": [
    "import random, hashlib\n",
    "\n",
    "def sample_files(report: dict, n: int = 3):\n",
    "    samples = {}\n",
    "    for name, entry in report.items():\n",
    "        if entry.get('type') == 'video':\n",
    "            files = sorted(Path(entry['path']).rglob('*.mp4'))\n",
    "            if files:\n",
    "                k = min(n, len(files))\n",
    "                samples[name] = random.sample(files, k)\n",
    "    return samples\n",
    "\n",
    "def sha256_limited(path: Path, max_bytes: int = 5 * 1024 * 1024):\n",
    "    h = hashlib.sha256()\n",
    "    total = 0\n",
    "    with path.open('rb') as f:\n",
    "        while total < max_bytes:\n",
    "            chunk = f.read(min(1024 * 1024, max_bytes - total))\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "            total += len(chunk)\n",
    "    return h.hexdigest(), total\n",
    "\n",
    "samples = sample_files(hr_inv, n=3)\n",
    "for name, files in samples.items():\n",
    "    print(f'{name} samples ({len(files)}):')\n",
    "    for p in files:\n",
    "        try:\n",
    "            size = p.stat().st_size\n",
    "        except Exception as exc:\n",
    "            print(' -', p, '-> stat failed:', exc)\n",
    "            continue\n",
    "        digest, scanned = sha256_limited(p)\n",
    "        print(f' - {p.name:40s} size={size:,}B sha256[:8]={digest[:8]} scanned={scanned:,}B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff93a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captionqa (3.10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
