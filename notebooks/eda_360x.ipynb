{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05ba053",
   "metadata": {},
   "source": [
    "# **360x Dataset EDA**\n",
    "\n",
    "Quick exploratory checks on the 360x panoramic dataset: verify structure, inspect sample entries, and capture basic summary stats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd72fae",
   "metadata": {},
   "source": [
    "### Dataset context\n",
    "- The [360+x project page](https://x360dataset.github.io/) describes the dataset as a **panoptic multi-modal scene understanding** corpus with 2,152 videos (8.579M frames) captured using 360° and Spectacles cameras across 17 cities in 5 countries, covering 28 scene categories spanning indoor and outdoor environments.\n",
    "- Each video carries **temporal activity localization for 38 action classes**, synchronized **binaural audio** (with published delay statistics), and aligned third-person, panoramic, and binocular viewpoints to encourage cross-modal analysis.\n",
    "- The downloadable packages on Hugging Face include both **high-resolution assets** (panoramic 5760×2880, binocular 2432×1216, front-view 1920×1080) and **lower-resolution variants** (panoramic/binocular 640×320, front-view 569×320), with shared JSON indices, class maps, and per-clip activity segmentation metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59231e7",
   "metadata": {},
   "source": [
    "## 1. Resolve dataset root\n",
    "\n",
    "Uses the same resolution logic as the quickstart notebook: prefer `CAPTIONQA_DATASETS`, otherwise search upward for the repository root and fall back to `./datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7b773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: D:\\CaptionQA\\data\n",
      "HR root: D:\\CaptionQA\\data\\360x\\360x_dataset_HR\n",
      "LR root: D:\\CaptionQA\\data\\360x\\360x_dataset_LR\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "import json\n",
    "import os\n",
    "\n",
    "def resolve_dataset_root() -> Path:\n",
    "    env_root = os.environ.get('CAPTIONQA_DATASETS')\n",
    "    if env_root:\n",
    "        return Path(env_root).expanduser().resolve()\n",
    "    cwd = Path.cwd()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        if (candidate / 'pyproject.toml').exists() or (candidate / '.git').exists():\n",
    "            return (candidate / 'datasets').resolve()\n",
    "    return (cwd / 'datasets').resolve()\n",
    "\n",
    "DATASET_ROOT = resolve_dataset_root()\n",
    "print(f'Dataset root: {DATASET_ROOT}')\n",
    "HR_ROOT = DATASET_ROOT / '360x' / '360x_dataset_HR'\n",
    "LR_ROOT = DATASET_ROOT / '360x' / '360x_dataset_LR'\n",
    "print(f'HR root: {HR_ROOT}')\n",
    "print(f'LR root: {LR_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ad13d",
   "metadata": {},
   "source": [
    "## 2. Validate presence & summarize\n",
    "\n",
    "Run the next cell to verify the expected folder layout and capture a few file listings. The notebook handles missing data gracefully and prints next steps if the dataset is absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e306c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High-resolution split -> D:\\CaptionQA\\data\\360x\\360x_dataset_HR\n",
      "Total entries: 5\n",
      " - .cache\n",
      " - .gitattributes\n",
      " - binocular\n",
      " - README.md\n",
      " - TAL_annotations\n",
      "\n",
      "Low-resolution split -> D:\\CaptionQA\\data\\360x\\360x_dataset_LR\n",
      "[missing] Not found. Download with: python -m captionqa.data.download 360x --output D:\\CaptionQA\\data\n"
     ]
    }
   ],
   "source": [
    "def describe_directory(root: Path, label: str, limit: int = 10):\n",
    "    print(f'\\n{label} -> {root}')\n",
    "    if not root.exists():\n",
    "        print('[missing] Not found. Download with: python -m captionqa.data.download 360x --output', DATASET_ROOT)\n",
    "        return\n",
    "    entries = sorted(root.iterdir())\n",
    "    print(f'Total entries: {len(entries)}')\n",
    "    for path in islice(entries, limit):\n",
    "        print(' -', path.name)\n",
    "    if len(entries) > limit:\n",
    "        print(' ...')\n",
    "\n",
    "describe_directory(HR_ROOT, 'High-resolution split')\n",
    "describe_directory(LR_ROOT, 'Low-resolution split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792ab4c",
   "metadata": {},
   "source": [
    "## 3. Sample metadata (optional)\n",
    "\n",
    "If JSON metadata files are available, the next cell attempts to load one sample to inspect structure. Adjust the glob pattern if the dataset uses a different naming scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762e86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing D:\\CaptionQA\\data\\360x\\360x_dataset_HR\\TAL_annotations\\019cc67f-512f-4b8a-96ef-81f806c86ce1.json\n",
      "{\n",
      "  \"file\": {\n",
      "    \"1\": {\n",
      "      \"fid\": \"1\",\n",
      "      \"fname\": \"360_panoramic.mp4\",\n",
      "      \"type\": 4,\n",
      "      \"loc\": 1,\n",
      "      \"src\": \"\"\n",
      "    }\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"1_1_1\": {\n",
      "      \"duration\": [\n",
      "        5.516,\n",
      "        7.3905\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"sitting\"\n",
      "      }\n",
      "    },\n",
      "    \"1_2_2\": {\n",
      "      \"duration\": [\n",
      "        12.312,\n",
      "        17.47384\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"drinking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_2_3\": {\n",
      "      \"duration\": [\n",
      "        51.417,\n",
      "        56.46075\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"drinking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_2_4\": {\n",
      "      \"duration\": [\n",
      "        235.334,\n",
      "        237.98142\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"drinking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_5\": {\n",
      "      \"duration\": [\n",
      "        9.558,\n",
      "        116.33559\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_6\": {\n",
      "      \"duration\": [\n",
      "        135.211,\n",
      "        167.50226\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_7\": {\n",
      "      \"duration\": [\n",
      "        174.288,\n",
      "        232.31476\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_8\": {\n",
      "      \"duration\": [\n",
      "        240.065,\n",
      "        271.08559\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_3_9\": {\n",
      "      \"duration\": [\n",
      "        274.359,\n",
      "        324.34302\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"speaking\"\n",
      "      }\n",
      "    },\n",
      "    \"1_4_10\": {\n",
      "      \"duration\": [\n",
      "        115.656,\n",
      "        313.96059\n",
      "      ],\n",
      "      \"action\": {\n",
      "        \"1\": \"operating phone\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "[skip] Root missing; nothing to load.\n"
     ]
    }
   ],
   "source": [
    "def load_sample_metadata(root: Path, suffix: str = '.json'):\n",
    "    if not root.exists():\n",
    "        print('[skip] Root missing; nothing to load.')\n",
    "        return\n",
    "    for path in sorted(root.rglob(f'*{suffix}')):\n",
    "        print(f'Previewing {path}')\n",
    "        try:\n",
    "            with path.open('r', encoding='utf-8') as handle:\n",
    "                snippet = json.load(handle)\n",
    "        except Exception as exc:\n",
    "            print('Failed to parse JSON:', exc)\n",
    "            return\n",
    "        print(json.dumps(snippet, indent=2)[:2000])\n",
    "        return\n",
    "    print('[skip] No files matching suffix found.')\n",
    "\n",
    "load_sample_metadata(HR_ROOT)\n",
    "load_sample_metadata(LR_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c42cd",
   "metadata": {},
   "source": [
    "## 4. Detailed roadmap for high-quality EDA\n",
    "\n",
    "### A. Data access & integrity audit\n",
    "1. **Confirm environment authentication**\n",
    "   - Verify `huggingface-cli whoami` succeeds so gated downloads remain accessible.\n",
    "   - Mirror the final dataset root (HR vs LR) and capture the exact path in the notebook for reproducibility.\n",
    "2. **Inventory archives & modality folders**\n",
    "   - Programmatically list `panoramic/`, `binocular/`, `third_person/`, and `activity_segmentation/` directories; assert counts match published totals (2,152 full videos / 1,380 clips) once decompressed.\n",
    "   - Parse `index.json` to extract per-item metadata (scene id, city, capture device, clip ids) and ensure all referenced files exist.\n",
    "3. **Checksum / size validation (spot checks)**\n",
    "   - Compute file sizes & optional hashes for a random stratified sample to confirm downloads are complete across modalities and resolutions.\n",
    "\n",
    "### B. Metadata profiling\n",
    "1. **Scene & geography coverage**\n",
    "   - Load `index.json` into a DataFrame; summarize counts by `scene_category`, `city`, `country`, indoor/outdoor flag to verify the 28-scene, 17-city distribution.\n",
    "   - Visualize distributions (bar charts, choropleth-ready tables) and highlight underrepresented categories.\n",
    "2. **Action label analysis**\n",
    "   - Iterate `activity_segmentation/*.json`; explode temporal segments to calculate frequency, duration, and co-occurrence of the 38 action classes.\n",
    "   - Plot duration histograms and per-video action counts to reproduce/double-check dataset charts (e.g., number of actions per clip, time-of-day coverage).\n",
    "3. **Clip segmentation review**\n",
    "   - Confirm 1,380 ~10s clips by aggregating segment metadata, checking total duration (~244k seconds / 67.78 hours) against expectations.\n",
    "\n",
    "### C. Video modality diagnostics\n",
    "1. **Resolution & bitrate verification**\n",
    "   - Use `ffprobe` (via `ffmpeg-python` or subprocess) on samples from each modality/resolution to confirm frame size, FPS, codec, audio channels.\n",
    "   - Tabulate metrics to ensure panoramic videos retain 360° projection metadata (e.g., equirectangular tags).\n",
    "2. **Temporal alignment checks**\n",
    "   - For matching clip IDs across panoramic/front-view/binocular videos, compute start/end timestamps and verify synchronization with activity segments.\n",
    "   - Overlay representative frames to inspect spatial correspondence between modalities.\n",
    "3. **Quality spotlights**\n",
    "   - Render thumbnails or short GIFs for a stratified sample (scene type × device) to visually inspect exposure, motion blur, and unique scenarios.\n",
    "\n",
    "### D. Audio & binaural analysis\n",
    "1. **Channel inspection**\n",
    "   - Confirm audio streams are stereo/binaural; measure inter-channel delay statistics to compare with published histograms.\n",
    "2. **Spectrogram profiling**\n",
    "   - Generate Mel spectrograms for random clips to assess frequency coverage; store representative figures in the notebook.\n",
    "3. **Cross-modal cues**\n",
    "   - Correlate audio energy bursts with action segment timestamps to evaluate labeling quality.\n",
    "\n",
    "### E. Feature & annotation validation\n",
    "1. **Pre-computed feature parity**\n",
    "   - If I3D/VGGish/ResNet-18 features are present, verify dimensionality and sample statistics; confirm number of feature files equals number of clips.\n",
    "2. **Class mapping sanity checks**\n",
    "   - Inspect `classes.json` to validate naming consistency between metadata and activity labels; flag missing or duplicate entries.\n",
    "\n",
    "### F. Documentation & reproducibility\n",
    "1. **Record assumptions & gaps**\n",
    "   - Maintain a running log within the notebook capturing any anomalies (missing files, corrupted clips) and remediation steps.\n",
    "2. **Outline next analytical directions**\n",
    "   - Based on findings, prioritize deeper tasks (e.g., pose estimation feasibility, QA pair synthesis), linking them to concrete dataset evidence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captionqa (3.10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
