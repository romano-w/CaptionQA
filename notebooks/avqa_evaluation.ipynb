{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVQA Zero-shot & Fine-tuned Evaluation\n",
    "\n",
    "This notebook demonstrates how to load a small subset of the [AVQA](https://github.com/AlyssaYoung/AVQA) dataset, instantiate the multimodal QA model implemented in `captionqa.qa`, and run both zero-shot and fine-tuned inference.\n",
    "\n",
    "Update the dataset path below to match your local environment (see the project README for the recommended `CAPTIONQA_DATASETS` layout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from captionqa.qa import load_avqa_subset\n",
    "\n",
    "DATASET_ROOT = Path(\"D:/CaptionQA/data/avqa/AVQA\")  # TODO: customise for your machine\n",
    "dataset = load_avqa_subset(DATASET_ROOT, split=\"val\", subset_size=4)\n",
    "print(f\"Loaded {len(dataset)} AVQA items from {DATASET_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, samples):\n",
    "        self.special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "        self.token_to_id = {tok: idx for idx, tok in enumerate(self.special_tokens)}\n",
    "        self.id_to_token = list(self.token_to_id.keys())\n",
    "        for sample in samples:\n",
    "            self._add_text(sample[\"question\"])\n",
    "            if sample.get(\"answer\"):\n",
    "                self._add_text(sample[\"answer\"])\n",
    "        self.pad_token_id = self.token_to_id[\"<pad>\"]\n",
    "        self.bos_token_id = self.token_to_id[\"<bos>\"]\n",
    "        self.eos_token_id = self.token_to_id[\"<eos>\"]\n",
    "\n",
    "    def _add_text(self, text):\n",
    "        for token in text.lower().split():\n",
    "            if token not in self.token_to_id:\n",
    "                self.token_to_id[token] = len(self.id_to_token)\n",
    "                self.id_to_token.append(token)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        tokens = []\n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.bos_token_id)\n",
    "        for token in text.lower().split():\n",
    "            tokens.append(self.token_to_id.get(token, self.pad_token_id))\n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.eos_token_id)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        words = []\n",
    "        for token_id in token_ids:\n",
    "            if skip_special_tokens and token_id < len(self.special_tokens):\n",
    "                continue\n",
    "            if token_id < len(self.id_to_token):\n",
    "                words.append(self.id_to_token[token_id])\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_to_token)\n",
    "\n",
    "tokenizer = SimpleTokenizer(dataset)\\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from captionqa.qa import AVQAModel, run_zero_shot\n",
    "\n",
    "model = AVQAModel(\n",
    "    video_dim=512,\n",
    "    audio_dim=256,\n",
    "    vocab_size=len(tokenizer),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "results = run_zero_shot(model, dataset, tokenizer, device=\"cpu\", max_length=8)\n",
    "for item in results:\n",
    "    print(f\"{item.question_id}: {item.prediction} (answer: {item.reference})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and provide a checkpoint file to evaluate a fine-tuned model\n",
    "# from captionqa.qa import run_fine_tuned\n",
    "# checkpoint_path = Path(\"path/to/avqa_checkpoint.pt\")\n",
    "# ft_results = run_fine_tuned(checkpoint_path, dataset, tokenizer, device=\"cuda\")\n",
    "# for item in ft_results:\n",
    "#     print(f\"{item.question_id}: {item.prediction} (answer: {item.reference})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
