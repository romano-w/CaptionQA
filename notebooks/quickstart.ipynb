{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6becd0",
   "metadata": {},
   "source": [
    "# CaptionQA Quickstart\n",
    "\n",
    "Welcome to the CaptionQA quickstart notebook. This guide helps you verify your development environment, explore the dataset utilities bundled with the repository, and establish a baseline workflow for panoramic captioning + QA research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec78cb",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "CaptionQA uses [uv](https://docs.astral.sh/uv/) for dependency management and targets Python 3.10+. On Windows 11 PowerShell, the recommended bootstrap sequence is:\n",
    "\n",
    "```powershell\n",
    "winget install --id Astral.Uv -e\n",
    "uv venv captionqa\n",
    ".\\captionqa\\Scripts\\Activate.ps1\n",
    "uv pip install --editable .\n",
    "```\n",
    "\n",
    "If you are working on macOS or Linux, the commands are identical except for the activation step (`source captionqa/bin/activate`).\n",
    "\n",
    "> **Tip:** Ensure that FFmpeg is installed and available on your `PATH` before attempting to process audio/video assets.\n",
    "\n",
    "Run the following cell to confirm the Python version and virtual environment information inside the notebook kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ca2fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.1\n",
      "Executable: c:\\Users\\willj\\Documents\\Coding Projects\\CaptionQA\\captionqa\\Scripts\\python.exe\n",
      "Platform: Windows-10-10.0.26100-SP0\n",
      "Virtual env: C:\\Users\\willj\\Documents\\Coding Projects\\CaptionQA\\captionqa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(f'Python version: {sys.version.split()[0]}')\n",
    "print(f'Executable: {sys.executable}')\n",
    "print(f'Platform: {platform.platform()}')\n",
    "print(f'Virtual env: {os.environ.get(\"VIRTUAL_ENV\", \"<none>\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c0de5",
   "metadata": {},
   "source": [
    "If the `Virtual env` field shows `<none>`, activate the `captionqa` environment (or your preferred venv) and restart the Jupyter kernel before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9ad98",
   "metadata": {},
   "source": [
    "## 2. Dataset utilities\n",
    "\n",
    "The `captionqa.data.download` module centralizes dataset metadata and provides a command-line interface. The next cell lists all datasets currently configured. This is a safe operation that does **not** download any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bcfa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: C:\\Users\\willj\\Documents\\Coding Projects\\CaptionQA\\notebooks\\datasets\n",
      "\n",
      "Available datasets:\n",
      "360x       - Panoramic video dataset with scene descriptions, action labels, and binaural audio.\n",
      "360dvd     - Dense 360° video understanding dataset for video-language modeling.\n",
      "leader360v - Large-scale 360° dataset for object tracking and viewpoint-aware understanding.\n",
      "360sr      - Static panoramic scene classification dataset for spatial scene context models.\n",
      "avqa       - Audio-visual question answering dataset repository with preprocessing utilities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willj\\Documents\\Coding Projects\\CaptionQA\\captionqa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from captionqa.data import DATASETS\n",
    "\n",
    "DATASET_ROOT = Path('datasets').expanduser().resolve()\n",
    "print(f'Dataset root: {DATASET_ROOT}')\n",
    "print('\\nAvailable datasets:')\n",
    "for name, task in DATASETS.items():\n",
    "    print(f'{name:<10s} - {task.description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d369b10",
   "metadata": {},
   "source": [
    "Use the CLI from a terminal to download assets once you have granted Hugging Face access where required:\n",
    "\n",
    "```bash\n",
    "python -m captionqa.data.download --list\n",
    "python -m captionqa.data.download 360x --output datasets --dry-run\n",
    "python -m captionqa.data.download leader360v --output datasets\n",
    "```\n",
    "\n",
    "The `--dry-run` option prints the operations without performing any downloads, which is useful for verifying credentials and paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d8835",
   "metadata": {},
   "source": [
    "## 3. Inspecting a downloaded dataset\n",
    "\n",
    "After downloading, you can explore the file structure programmatically. The example below demonstrates how to enumerate the top-level contents of the 360x dataset. If the dataset is not yet downloaded, the code will notify you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e97f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360x high-resolution dataset not found at C:\\Users\\willj\\Documents\\Coding Projects\\CaptionQA\\notebooks\\datasets\\360x\\360x_dataset_HR. Run the downloader once you have access.\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "dataset_root = globals().get('DATASET_ROOT')\n",
    "if dataset_root is None:\n",
    "    from pathlib import Path\n",
    "    dataset_root = Path('datasets').expanduser().resolve()\n",
    "\n",
    "hr_root = dataset_root / '360x' / '360x_dataset_HR'\n",
    "if hr_root.exists():\n",
    "    entries = sorted(hr_root.iterdir())\n",
    "    print(f'Found {len(entries)} items under {hr_root}:')\n",
    "    for path in islice(entries, 10):\n",
    "        print(' -', path.relative_to(hr_root))\n",
    "    if len(entries) > 10:\n",
    "        print('...')\n",
    "else:\n",
    "    print(f'360x high-resolution dataset not found at {hr_root}. Run the downloader once you have access.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23f79b",
   "metadata": {},
   "source": [
    "Repeat the pattern for other datasets—adjust the root path and traversal depth depending on the structure (archives vs. Git repositories)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be30e56",
   "metadata": {},
   "source": [
    "## 4. Next steps\n",
    "\n",
    "* Prototype captioning models using your framework of choice (e.g., PyTorch + Hugging Face Transformers).\n",
    "* Ingest panoramic video clips and convert them into frame sequences or features suitable for model training.\n",
    "* Integrate QA annotations by aligning temporal segments with generated captions.\n",
    "\n",
    "This notebook will continue to evolve as the project matures. Feel free to add exploratory experiments, preprocessing utilities, and evaluation routines in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18f50a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captionqa (3.10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
